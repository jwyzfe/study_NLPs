{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers datasets huggingface_hub fsspec==2024.10.0 -qqq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T06:49:21.206155Z","iopub.execute_input":"2025-02-04T06:49:21.206496Z","iopub.status.idle":"2025-02-04T06:49:32.261103Z","shell.execute_reply.started":"2025-02-04T06:49:21.206458Z","shell.execute_reply":"2025-02-04T06:49:32.259360Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\ns3fs 2024.9.0 requires fsspec==2024.9.0.*, but you have fsspec 2024.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Hugging face 사용 이유\n- 여러 회사들이 개발한 모델이나 제공하는 데이터셋을 편리하게 사용하는 인터페이스 제공","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T06:53:35.681422Z","iopub.execute_input":"2025-02-04T06:53:35.681782Z","iopub.status.idle":"2025-02-04T06:53:47.319424Z","shell.execute_reply.started":"2025-02-04T06:53:35.681749Z","shell.execute_reply":"2025-02-04T06:53:47.318443Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Bert 모델 활용 준비\n- https://huggingface.co/google-bert/bert-base-uncased\n","metadata":{}},{"cell_type":"code","source":"bert_model = AutoModel.from_pretrained('google-bert/bert-base-uncased')\nbert_tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T06:59:43.297438Z","iopub.execute_input":"2025-02-04T06:59:43.297768Z","iopub.status.idle":"2025-02-04T06:59:43.631352Z","shell.execute_reply.started":"2025-02-04T06:59:43.297745Z","shell.execute_reply":"2025-02-04T06:59:43.630417Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"type(bert_model), type(bert_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T06:59:56.896881Z","iopub.execute_input":"2025-02-04T06:59:56.897225Z","iopub.status.idle":"2025-02-04T06:59:56.904169Z","shell.execute_reply.started":"2025-02-04T06:59:56.897199Z","shell.execute_reply":"2025-02-04T06:59:56.903006Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(transformers.models.bert.modeling_bert.BertModel,\n transformers.models.bert.tokenization_bert_fast.BertTokenizerFast)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"text = 'What is Huggingface Transformers ?'\nencoded_input = bert_tokenizer(text, return_tensors='pt')# 학습 완료된 형태에 벡터로 결과 리턴\nbert_out = bert_model(**encoded_input)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:06:01.817434Z","iopub.execute_input":"2025-02-04T07:06:01.817808Z","iopub.status.idle":"2025-02-04T07:06:02.071207Z","shell.execute_reply.started":"2025-02-04T07:06:01.817779Z","shell.execute_reply":"2025-02-04T07:06:02.069326Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# type(encoded_input), encoded_input\ntype(bert_out), bert_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:06:03.379255Z","iopub.execute_input":"2025-02-04T07:06:03.379674Z","iopub.status.idle":"2025-02-04T07:06:03.427261Z","shell.execute_reply.started":"2025-02-04T07:06:03.379645Z","shell.execute_reply":"2025-02-04T07:06:03.425823Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions,\n BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3009,  0.0158,  0.0698,  ..., -0.3406,  0.5976,  0.5820],\n          [-0.1109,  0.0754, -0.1906,  ...,  0.2970,  0.4278, -0.0391],\n          [-0.5813, -0.0042,  0.4034,  ..., -0.2549,  0.2216,  0.8121],\n          ...,\n          [ 0.9971,  0.3301, -0.0688,  ..., -0.4873,  0.0168, -0.0345],\n          [-0.2394, -0.0573, -0.5885,  ..., -0.0415,  0.3123, -0.0288],\n          [ 0.7884,  0.4039,  0.0217,  ...,  0.3869, -0.4785, -0.4116]]],\n        grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.7247e-01, -3.1464e-01, -5.2734e-01,  7.3685e-01,  1.8460e-01,\n          -1.8295e-01,  8.9985e-01,  2.9762e-01, -3.9314e-01, -9.9994e-01,\n           7.1271e-02,  7.2618e-01,  9.7232e-01,  3.2968e-01,  9.1370e-01,\n          -7.5043e-01, -3.5754e-01, -5.6530e-01,  2.8968e-01, -6.4040e-01,\n           6.0199e-01,  9.9856e-01,  3.8675e-01,  2.9078e-01,  4.0431e-01,\n           7.7010e-01, -7.6515e-01,  9.1830e-01,  9.4834e-01,  7.2557e-01,\n          -7.3735e-01,  1.9867e-01, -9.8172e-01, -1.8334e-01, -4.9430e-01,\n          -9.8059e-01,  2.6051e-01, -7.4618e-01,  4.8118e-02,  5.9711e-02,\n          -8.1904e-01,  2.1572e-01,  9.9974e-01, -6.2062e-01,  1.3181e-02,\n          -3.7181e-01, -1.0000e+00,  2.5721e-01, -8.5921e-01,  3.5142e-01,\n           4.2222e-01,  2.7745e-02,  1.5076e-01,  3.6331e-01,  4.4038e-01,\n           2.4007e-02, -1.7561e-01,  6.5655e-02, -2.2944e-01, -5.1775e-01,\n          -6.1054e-01,  4.0539e-01, -4.9569e-01, -8.7416e-01,  5.6503e-01,\n           5.4685e-01, -9.5152e-02, -2.8588e-01, -6.3586e-02, -5.9148e-02,\n           8.8904e-01,  1.0488e-01,  2.8848e-01, -8.3510e-01,  1.3450e-01,\n           2.2096e-01, -6.5445e-01,  1.0000e+00, -5.5778e-01, -9.5749e-01,\n           4.4432e-01,  5.2114e-01,  5.8760e-01,  2.3120e-02,  3.6001e-01,\n          -1.0000e+00,  2.6260e-01, -6.2052e-02, -9.8500e-01,  2.5122e-01,\n           4.3635e-01, -1.6588e-01,  3.9133e-01,  5.3988e-01, -5.7578e-01,\n          -1.7328e-01, -8.3042e-02, -3.6534e-01, -1.3268e-01, -1.2891e-01,\n           2.1034e-02, -2.0756e-01, -2.4547e-01, -3.3774e-01,  2.4423e-01,\n          -4.7804e-01, -4.5389e-01,  3.9510e-01,  7.4780e-02,  6.1011e-01,\n           5.0728e-01, -3.0130e-01,  3.7723e-01, -9.3758e-01,  4.9906e-01,\n          -3.2958e-01, -9.8185e-01, -6.1150e-01, -9.7898e-01,  6.7381e-01,\n          -6.0385e-02, -2.6210e-01,  9.3328e-01,  1.9747e-01,  3.5069e-01,\n          -7.0666e-02, -3.4704e-01, -1.0000e+00, -2.7822e-01, -3.7445e-01,\n          -1.7645e-01, -2.1203e-01, -9.6432e-01, -9.4622e-01,  5.7054e-01,\n           9.4001e-01,  1.8941e-01,  9.9923e-01, -1.5256e-01,  9.1510e-01,\n           7.8498e-02, -5.6504e-01, -7.2290e-02, -4.5555e-01,  5.5808e-01,\n           1.8401e-01, -6.0199e-01,  2.3481e-01, -3.3689e-02, -2.7570e-01,\n          -5.3239e-01, -1.7641e-01, -4.0169e-01, -9.2079e-01, -3.1998e-01,\n           9.3809e-01, -7.1153e-02, -6.9783e-01,  6.7252e-02, -2.0274e-01,\n          -3.7808e-01,  8.4576e-01,  5.0007e-01,  3.6423e-01, -4.1740e-01,\n           3.6109e-01,  1.4876e-01,  3.5860e-01, -8.4838e-01,  2.6076e-01,\n           4.1683e-01, -3.1267e-01, -5.2371e-01, -9.6102e-01, -2.9391e-01,\n           5.4447e-01,  9.8303e-01,  7.4894e-01,  2.8395e-01,  1.4951e-01,\n          -2.6745e-01,  1.8009e-01, -9.3940e-01,  9.6656e-01, -1.0958e-01,\n           1.8235e-01,  1.6496e-01,  2.4865e-01, -8.2815e-01,  7.0506e-04,\n           8.1065e-01, -1.2265e-02, -8.1630e-01, -4.3831e-02, -4.7633e-01,\n          -3.8496e-01, -3.8118e-01,  4.5080e-01, -1.9782e-01, -3.4960e-01,\n           1.1252e-01,  8.9026e-01,  9.7578e-01,  6.9775e-01, -1.2651e-01,\n           5.7326e-01, -8.3355e-01, -4.0410e-01,  5.5685e-02,  1.7855e-01,\n           8.9321e-02,  9.8696e-01, -3.9309e-01, -7.7632e-02, -9.1122e-01,\n          -9.7244e-01,  5.5041e-03, -8.4574e-01, -2.4718e-02, -6.3462e-01,\n           4.9246e-01,  5.5000e-01,  1.1419e-01,  3.0862e-01, -9.6484e-01,\n          -7.7138e-01,  3.5477e-01, -2.1973e-01,  4.2417e-01, -2.4453e-01,\n           6.6965e-01,  6.5434e-01, -6.3527e-01,  8.0241e-01,  9.1417e-01,\n          -4.4787e-01, -6.7925e-01,  8.3256e-01, -2.6945e-01,  8.7207e-01,\n          -5.2281e-01,  9.8420e-01,  4.2696e-01,  4.8134e-01, -8.7702e-01,\n          -5.7352e-01, -8.7672e-01, -3.6707e-01,  7.0802e-02, -1.4201e-01,\n           6.3290e-01,  6.3346e-01,  2.9797e-01,  4.1055e-01, -6.0411e-01,\n           9.9386e-01, -7.6780e-01, -9.4760e-01,  2.8060e-01,  1.7029e-02,\n          -9.8417e-01,  3.4940e-01,  2.5224e-01, -4.0932e-01, -3.8472e-01,\n          -6.1061e-01, -9.4425e-01,  8.4996e-01,  1.8030e-01,  9.8473e-01,\n          -8.3626e-02, -8.9589e-01, -4.1394e-01, -8.9448e-01, -2.8790e-01,\n          -1.7622e-01,  1.2934e-01, -1.4377e-01, -9.4701e-01,  4.8915e-01,\n           5.5083e-01,  3.5019e-01, -5.7310e-01,  9.9580e-01,  9.9998e-01,\n           9.5247e-01,  8.9147e-01,  8.6635e-01, -9.9394e-01, -3.3779e-01,\n           9.9998e-01, -9.1078e-01, -1.0000e+00, -9.3294e-01, -5.2658e-01,\n           3.7828e-01, -1.0000e+00, -3.1776e-01,  7.3381e-02, -8.6886e-01,\n           3.6298e-01,  9.6532e-01,  9.8247e-01, -1.0000e+00,  8.0954e-01,\n           9.0122e-01, -6.6414e-01,  6.7350e-01, -3.1281e-01,  9.6308e-01,\n           5.0662e-01,  4.2693e-01, -2.2366e-01,  3.7596e-01, -8.1644e-01,\n          -8.3308e-01, -1.3696e-01, -4.6714e-01,  9.4581e-01,  1.8325e-01,\n          -7.8852e-01, -8.7354e-01, -5.8152e-02, -6.9013e-02, -3.4173e-01,\n          -9.4760e-01, -1.7820e-01, -1.3928e-01,  6.8355e-01,  4.5169e-02,\n           2.6814e-01, -6.8769e-01,  2.1210e-01, -2.3396e-01,  3.4735e-01,\n           6.8611e-01, -9.2963e-01, -5.2597e-01,  3.0077e-01, -3.7891e-01,\n          -4.7398e-01, -9.5464e-01,  9.4878e-01, -3.2371e-01,  3.0548e-01,\n           1.0000e+00, -1.6540e-01, -7.9525e-01,  5.4072e-01,  1.6751e-01,\n          -1.2475e-01,  1.0000e+00,  6.4986e-01, -9.5677e-01, -6.0413e-01,\n           3.7020e-01, -4.8581e-01, -4.7777e-01,  9.9866e-01, -2.5729e-01,\n          -2.4092e-01,  1.1121e-01,  9.6287e-01, -9.8189e-01,  9.3767e-01,\n          -8.7587e-01, -9.4305e-01,  9.3710e-01,  9.0091e-01, -3.8188e-01,\n          -6.1976e-01,  1.4977e-01, -5.8692e-01,  2.6118e-01, -9.5406e-01,\n           6.2279e-01,  4.8689e-01, -8.8540e-02,  8.5752e-01, -8.3492e-01,\n          -5.3720e-01,  2.5680e-01, -4.1266e-01,  2.2846e-01,  5.8122e-01,\n           4.3429e-01, -2.4578e-01,  1.2280e-01, -2.9919e-01, -3.1362e-01,\n          -9.6913e-01,  6.2339e-02,  1.0000e+00,  1.3627e-02, -1.2816e-01,\n          -3.8745e-01,  1.8794e-02, -1.8447e-01,  4.2568e-01,  5.1443e-01,\n          -2.2532e-01, -8.0441e-01,  1.4079e-01, -9.4743e-01, -9.8253e-01,\n           7.6432e-01,  1.4451e-01, -3.2291e-01,  9.9988e-01,  3.1823e-01,\n           1.5380e-01,  1.3672e-01,  7.6152e-01,  1.0233e-01,  5.8834e-01,\n           5.5959e-01,  9.6941e-01, -2.7468e-01,  6.1814e-01,  8.2141e-01,\n          -5.6555e-01, -2.7680e-01, -6.2172e-01, -8.9432e-02, -9.0074e-01,\n           1.2009e-01, -9.2751e-01,  9.4329e-01,  4.9252e-01,  2.7484e-01,\n           2.1843e-01,  6.6066e-02,  1.0000e+00, -2.2593e-01,  6.4518e-01,\n          -5.1134e-01,  8.2547e-01, -9.8957e-01, -7.8994e-01, -3.4983e-01,\n          -8.3619e-02, -3.6136e-01, -2.2800e-01,  2.0661e-01, -9.5846e-01,\n           3.5442e-01,  9.7110e-02, -9.7418e-01, -9.8264e-01,  2.9154e-01,\n           8.3737e-01,  6.6634e-02, -8.0501e-01, -6.1920e-01, -5.9091e-01,\n           1.7083e-01, -1.7838e-01, -9.0619e-01,  2.4375e-01, -1.7610e-01,\n           4.1765e-01, -2.2214e-01,  5.1794e-01,  5.8822e-01,  6.4995e-01,\n           1.3124e-01, -9.7190e-02, -8.3229e-02, -8.4713e-01,  8.4501e-01,\n          -7.9905e-01, -4.0447e-01, -2.0988e-01,  1.0000e+00, -5.1438e-01,\n           6.3233e-01,  7.9540e-01,  6.0073e-01, -1.3826e-01,  2.1753e-01,\n           6.5091e-01,  1.4618e-01, -6.2027e-01, -4.0101e-01, -7.0176e-01,\n          -3.3691e-01,  6.5601e-01, -8.1211e-02,  4.6828e-01,  6.5125e-01,\n           4.2083e-01,  1.4356e-01, -3.9187e-02, -4.4730e-02,  9.9797e-01,\n          -1.2616e-01,  5.6322e-02, -5.0923e-01,  7.9198e-03, -3.3530e-01,\n          -4.9022e-01,  1.0000e+00,  2.7262e-01, -1.3420e-01, -9.8275e-01,\n          -3.7159e-01, -8.9745e-01,  9.9994e-01,  8.0115e-01, -7.2103e-01,\n           6.0988e-01,  2.2475e-01, -1.7136e-01,  7.3667e-01, -1.3005e-01,\n          -2.5390e-01,  2.7347e-01,  9.6592e-02,  9.2964e-01, -4.7674e-01,\n          -9.5305e-01, -6.8485e-01,  3.5561e-01, -9.3918e-01,  9.9616e-01,\n          -4.4200e-01, -1.8582e-01, -4.1399e-01,  4.5030e-01,  6.3905e-01,\n          -1.7178e-01, -9.7449e-01, -5.1296e-02,  1.6738e-01,  9.3006e-01,\n           1.3585e-01, -5.9081e-01, -9.1515e-01,  1.1425e-01,  4.2950e-01,\n          -5.2346e-01, -8.7475e-01,  9.4331e-01, -9.7478e-01,  3.9603e-01,\n           1.0000e+00,  3.9404e-01, -6.0146e-01,  2.9374e-02, -5.1601e-01,\n           2.4861e-01,  2.4923e-01,  6.7270e-01, -9.2070e-01, -2.8287e-01,\n          -7.6749e-02,  1.7067e-01, -8.0916e-02,  1.2053e-01,  5.9641e-01,\n           1.7944e-01, -5.4069e-01, -5.6923e-01, -2.5089e-02,  4.6722e-01,\n           8.3220e-01, -3.5286e-01, -1.6219e-01,  9.4051e-02, -9.4971e-02,\n          -8.7677e-01, -2.7396e-01, -2.1794e-01, -9.9938e-01,  7.5556e-01,\n          -1.0000e+00, -1.7111e-01, -1.8512e-01, -2.3656e-01,  7.9198e-01,\n           2.8102e-01,  2.7826e-01, -6.7673e-01, -5.1072e-01,  6.6396e-01,\n           6.7959e-01, -1.5749e-01,  1.9009e-01, -6.7916e-01,  1.1148e-01,\n          -4.1110e-03, -7.4852e-03, -1.5137e-01,  8.2224e-01, -1.6503e-01,\n           1.0000e+00,  1.4966e-01, -6.3775e-01, -9.6420e-01,  2.2951e-01,\n          -2.5219e-01,  1.0000e+00, -9.1262e-01, -9.3021e-01,  2.5157e-01,\n          -6.5498e-01, -8.3972e-01,  2.0794e-01,  1.4597e-01, -5.7222e-01,\n          -5.5899e-01,  9.3297e-01,  9.1276e-01, -6.3061e-01,  3.3351e-01,\n          -3.3155e-01, -3.4310e-01,  5.5119e-02,  1.7480e-01,  9.7497e-01,\n           1.8068e-01,  8.6350e-01,  4.1295e-01,  6.8413e-02,  9.4591e-01,\n           2.3672e-01,  4.7458e-01,  5.9380e-02,  1.0000e+00,  2.6528e-01,\n          -8.8648e-01,  3.4260e-01, -9.7573e-01, -1.5098e-01, -9.4319e-01,\n           2.2633e-01,  2.1388e-01,  8.7123e-01, -9.6188e-02,  9.3925e-01,\n           2.4523e-02, -9.8376e-02, -1.0092e-01,  1.4681e-01,  3.8430e-01,\n          -8.7014e-01, -9.7812e-01, -9.7528e-01,  3.6960e-01, -3.8164e-01,\n          -4.7706e-02,  1.4460e-01,  1.0588e-01,  3.0843e-01,  4.1187e-01,\n          -1.0000e+00,  9.1188e-01,  3.6029e-01,  7.0125e-01,  9.3118e-01,\n           5.1771e-01,  2.7587e-01,  2.8873e-01, -9.7848e-01, -9.4348e-01,\n          -3.1992e-01, -2.0700e-01,  6.7060e-01,  6.4204e-01,  8.7050e-01,\n           3.0572e-01, -4.9178e-01, -2.6046e-01,  1.0768e-01, -5.5766e-01,\n          -9.8769e-01,  3.4500e-01, -1.2493e-01, -9.4611e-01,  9.4435e-01,\n          -2.2333e-01, -1.4135e-01,  1.9776e-01, -3.9046e-01,  9.3880e-01,\n           6.8928e-01,  3.8767e-01,  1.1025e-01,  5.1922e-01,  8.3273e-01,\n           9.4730e-01,  9.7341e-01, -5.7427e-01,  7.8560e-01, -6.8622e-02,\n           4.1192e-01,  6.9466e-01, -9.3774e-01,  2.3971e-01,  1.3474e-01,\n          -4.2771e-01,  1.6046e-01, -2.4939e-01, -9.7068e-01,  5.0688e-01,\n          -2.9860e-01,  5.1771e-01, -3.1591e-01,  1.0868e-01, -3.8843e-01,\n          -1.5632e-01, -6.4045e-01, -5.4585e-01,  6.6964e-01,  3.3962e-01,\n           8.5658e-01,  5.4788e-01, -1.0526e-01, -5.1888e-01, -1.6210e-01,\n          -5.4360e-01, -8.8314e-01,  8.7812e-01,  4.3410e-02, -2.1279e-01,\n           3.2289e-01, -1.8518e-01,  6.5461e-01, -6.4462e-02, -3.7191e-01,\n          -3.7232e-01, -7.5502e-01,  8.0540e-01, -1.7700e-01, -5.2501e-01,\n          -5.6918e-01,  3.7711e-01,  3.8391e-01,  9.9864e-01, -5.3484e-01,\n          -5.4315e-01, -7.2385e-02, -2.4831e-01,  4.2074e-01, -2.2462e-01,\n          -1.0000e+00,  2.4747e-01,  7.5571e-02,  3.7574e-01, -7.1565e-02,\n           3.3389e-01, -1.6045e-01, -9.6561e-01, -1.6314e-01,  7.3814e-02,\n           3.5186e-01, -4.4838e-01, -2.2300e-01,  5.7876e-01,  6.1218e-01,\n           7.0053e-01,  8.2005e-01,  1.1350e-01,  2.0501e-01,  6.4786e-01,\n          -2.2238e-01, -6.3388e-01,  8.9791e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None))"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### 모델 불러오기\n- 모델 : 바디와 헤드로 구분","metadata":{}},{"cell_type":"code","source":"from transfor\n# 바디만 있는 모델\n# https://huggingface.co/klue/roberta-base\nroberta_model = AutoModel.from_pretrained('klue/roberta-base')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:41:19.983854Z","iopub.execute_input":"2025-02-04T07:41:19.984205Z","iopub.status.idle":"2025-02-04T07:41:22.610953Z","shell.execute_reply.started":"2025-02-04T07:41:19.984178Z","shell.execute_reply":"2025-02-04T07:41:22.609526Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ca170d52bcb437f95ad8b8c5c335030"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36aff5b8b51841c68385bfe99fb1f1a7"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# 바디와 헤드가 포함된 모델\n#https://huggingface.co/SamLowe/roberta-base-go_emotions\nfrom transformers import AutoModelForSequenceClassification\nroberta_go_emotions_model = AutoModel.from_pretrained('SamLowe/roberta-base-go_emotions')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:52:34.241915Z","iopub.execute_input":"2025-02-04T07:52:34.242289Z","iopub.status.idle":"2025-02-04T07:52:34.424246Z","shell.execute_reply.started":"2025-02-04T07:52:34.242260Z","shell.execute_reply":"2025-02-04T07:52:34.423087Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"roberta_go_emotions_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:52:47.643103Z","iopub.execute_input":"2025-02-04T07:52:47.643482Z","iopub.status.idle":"2025-02-04T07:52:47.651471Z","shell.execute_reply.started":"2025-02-04T07:52:47.643453Z","shell.execute_reply":"2025-02-04T07:52:47.650251Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# 헤드에 없는 모델을 목적에 의해 불러오기\nroberta_classification_model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-base')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:53:28.472018Z","iopub.execute_input":"2025-02-04T07:53:28.472361Z","iopub.status.idle":"2025-02-04T07:53:28.646665Z","shell.execute_reply.started":"2025-02-04T07:53:28.472335Z","shell.execute_reply":"2025-02-04T07:53:28.645497Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"roberta_classification_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:53:30.727216Z","iopub.execute_input":"2025-02-04T07:53:30.727613Z","iopub.status.idle":"2025-02-04T07:53:30.735953Z","shell.execute_reply.started":"2025-02-04T07:53:30.727584Z","shell.execute_reply":"2025-02-04T07:53:30.734831Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}